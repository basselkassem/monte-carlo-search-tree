{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCTreeSearsh.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Wuu3YoJMiBk4",
        "eVHOYHM0iPDI"
      ],
      "authorship_tag": "ABX9TyNAyo1gYmOvrm85nktzh3tw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basselkassem/monte-carlo-search-tree/blob/master/MCTreeSearsh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfW9e05JcxLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from queue import Queue, LifoQueue\n",
        "from copy import copy\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        #print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysu3U04-eLZM",
        "colab_type": "code",
        "outputId": "46f0fb0a-2b20-4d7d-f889-39f17dea84ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "env = gym.make('Taxi-v3').env\n",
        "init_state = env.reset()\n",
        "env.render()\n",
        "\n",
        "action_num = env.action_space.n\n",
        "state_num = env.observation_space.n\n",
        "\n",
        "print('Action space:', action_num)\n",
        "print('Observation space:', state_num)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action space: 6\n",
            "Observation space: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHSpmBzxuW1C",
        "colab_type": "code",
        "outputId": "5820421d-769a-4aa6-8065-a6d71f2a3097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "state_id = 328\n",
        "env.s = state_id\n",
        "env.render()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A94Qa-OGqURf",
        "colab_type": "code",
        "outputId": "db353e86-1b60-4121-997b-b3fdd0cf2038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "env.P[state_id]\n",
        "#{action: [(probability, nextstate, reward, done)]}"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNtFKerJh9iq",
        "colab_type": "text"
      },
      "source": [
        "# Random policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vchiL2_fizT",
        "colab_type": "code",
        "outputId": "cb65d1eb-ed52-46a1-c07f-1c57f793a5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "is_done = False\n",
        "total_reward, penalty, epochs = 0, 0, 0\n",
        "while not is_done:\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  new_state, reward, is_done, info = env.step(action)\n",
        "\n",
        "  total_reward += reward\n",
        "  if reward == -10:\n",
        "    penalty += 1\n",
        "  epochs += 1\n",
        "\n",
        "env.render()\n",
        "print('Timesteps taken:', epochs)\n",
        "print('Penalty:', penalty)\n",
        "print('total_reward:', total_reward)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Timesteps taken: 3547\n",
            "Penalty: 1169\n",
            "total_reward: -14047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuu3YoJMiBk4",
        "colab_type": "text"
      },
      "source": [
        "#Planning with BFS & DFS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUPlWxTBtXK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "\n",
        "  def __init__(self, parent, env):\n",
        "    self.visited_times = 0\n",
        "    self.parent = parent\n",
        "    self.children = set()\n",
        "    self.observation = None\n",
        "    self.reward = 0\n",
        "    self.is_done = False\n",
        "    self.env_space = None\n",
        "    self.init_env_space(env)\n",
        "  \n",
        "  def init_env_space(self, env):\n",
        "    self.env_space = copy.deepcopy(env)\n",
        "\n",
        "  def is_terminal(self):\n",
        "    return self.is_done\n",
        "  \n",
        "  def get_env_space(self, action, parent_env):\n",
        "    self.observation, self.reward, self.is_done, _ = parent_env.step(action)\n",
        "    return parent_env\n",
        "\n",
        "  def expand(self):\n",
        "    for action in range(0, env.action_space.n):\n",
        "      parent_env = copy.deepcopy(self.env_space)\n",
        "      child_env_space = self.get_env_space(action, parent_env)\n",
        "      child_env_space.render()\n",
        "      print(action, self.is_done)\n",
        "      child = Node(self, child_env_space)\n",
        "      self.children.add(child)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Ml8ccKc11G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def planning_BFS(root):\n",
        "  total_reward, penalty, epochs = 0, 0, 0\n",
        "  queue = Queue()\n",
        " \n",
        "  root.visited_times += 1\n",
        "  queue.put(root)\n",
        "\n",
        "  while not queue.empty():\n",
        "    current_node = queue.get()\n",
        "    total_reward += current_node.reward\n",
        "    if current_node.reward == -10:\n",
        "      penalty += 1\n",
        "    epochs += 1\n",
        "    current_node.expand()\n",
        "    if current_node.is_done == True:\n",
        "      return {\n",
        "          'node': current_node,\n",
        "          'penalty': penalty,\n",
        "          'reward': total_reward,\n",
        "          'time_steps': epochs,\n",
        "      }\n",
        "    for child_node in current_node.children:\n",
        "      if child_node.visited_times == 0:\n",
        "        child_node.visited_times += 1\n",
        "        queue.put(child_node)\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mai0DloG9fb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def planning_DFS(root):\n",
        "  total_reward, penalty, epochs = 0, 0, 0\n",
        "  stack = LifoQueue()\n",
        "\n",
        "  root.visited_times += 1\n",
        "  stack.put(root)\n",
        "\n",
        "  while not stack.empty():\n",
        "    current_node = stack.get()\n",
        "    total_reward += current_node.reward\n",
        "    if current_node.reward == -10:\n",
        "      penalty += 1\n",
        "    epochs += 1\n",
        "    current_node.expand()\n",
        "     if current_node.is_done == True:\n",
        "      return {\n",
        "          'node': current_node,\n",
        "          'penalty': penalty,\n",
        "          'reward': total_reward,\n",
        "          'time_steps': epochs,\n",
        "      }\n",
        "    for child_node in current_node.children:\n",
        "      if child_node.visited_times == 0:\n",
        "        child_node.visited_times += 1\n",
        "        stack.put(child_node)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRffLVvV-mWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_path(node):\n",
        "  solutions = []\n",
        "  temp_node = node['node']\n",
        "  while temp_node.parent != None:\n",
        "    solutions.append(temp_node)\n",
        "    temp_node = temp_node.parent\n",
        "  return reversed(solutions)\n",
        "\n",
        "solutions = construct_path(termianl)\n",
        "for sol in solutions:\n",
        "  #clear_output(wait=True)\n",
        "  env.s = sol.observation\n",
        "  env.render()\n",
        "  #sleep(.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVHOYHM0iPDI",
        "colab_type": "text"
      },
      "source": [
        "# Learning Action-value Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXyi01q6xerP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.6\n",
        "alpha = 0.1\n",
        "epsilon = 0.1\n",
        "\n",
        "def epsilon_greedy(q_values, state, epsilon):\n",
        "  action = None\n",
        "  if np.random.uniform(0, 1) < epsilon:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = np.argmax(q_values[state])\n",
        "  return action\n",
        "\n",
        "def q_learning(iter_num):\n",
        "  q_values = np.zeros([state_num, action_num])\n",
        "  print(q_values.shape)\n",
        "  for i in range(iter_num):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = epsilon_greedy(q_values, state, epsilon)\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      q_values[state, action] += alpha * (reward + gamma * np.max(q_values[next_state]) - q_values[state, action])\n",
        "      state = next_state\n",
        "    if i % 100 == 0:\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Episode: {i}\")\n",
        "  print(\"Training finished.\\n\")\n",
        "  return q_values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I6-lWpf1w0a",
        "colab_type": "code",
        "outputId": "4e1614ae-8587-43fb-fc4a-bfb59d26e97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "q_policy = q_learning(10000)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 9900\n",
            "Training finished.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPtFLi3Y3u2k",
        "colab_type": "code",
        "outputId": "71ae0bb6-e346-4f90-8ad5-cab93ce379f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "is_done = False\n",
        "total_reward, penalty, epochs = 0, 0, 0\n",
        "frames = []\n",
        "state = env.reset()\n",
        "env.render()\n",
        "while not is_done:\n",
        "  action = np.argmax(q_policy[state])\n",
        "  state, reward, is_done, info = env.step(action)\n",
        "\n",
        "  total_reward += reward\n",
        "  if reward == -10:\n",
        "    penalty += 1\n",
        "  epochs += 1\n",
        "print('Timesteps taken:', epochs)\n",
        "print('Penalty:', penalty)\n",
        "print('total_reward:', total_reward)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Timesteps taken: 13\n",
            "Penalty: 0\n",
            "total_reward: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbazRm1_SNdK",
        "colab_type": "text"
      },
      "source": [
        "# MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv80m3vKSPmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "\n",
        "  def __init__(self, env, parent = None):\n",
        "    self.state = env\n",
        "    self.parent = parent\n",
        "    self.children = []\n",
        "    self.untried_actions = [action for action in range(action_num)]\n",
        "    self.visiting_times = 0\n",
        "    self.q = 0\n",
        "    self.is_done = False\n",
        "    self.observation = None\n",
        "    self.reward = 0\n",
        "    self.action = None\n",
        "\n",
        "  def is_fully_expanded(self):\n",
        "    return len(self.untried_actions) == 0\n",
        "\n",
        "  def is_terminal_node(self):\n",
        "    return self.is_done\n",
        "\n",
        "  def compute_mean_value(self):\n",
        "    if self.visiting_times == 0:\n",
        "      return 0\n",
        "    return self.q / self.visiting_times\n",
        "\n",
        "  def compute_score(self, scale = 10, max_score = 10e100):\n",
        "    if self.visiting_times == 0:\n",
        "      return max_score\n",
        "    parent_visiting_times = self.parent.visiting_times\n",
        "    ucb = 2 * np.sqrt(np.log(parent_visiting_times) / self.visiting_times)\n",
        "    result = self.compute_mean_value() + scale * ucb\n",
        "    return result\n",
        "\n",
        "  def best_child(self):\n",
        "    scores = [child.compute_score() for child in self.children]\n",
        "    child_index = np.argmax(scores)\n",
        "    return self.children[child_index]\n",
        "\n",
        "  def expand(self):\n",
        "    action = self.untried_actions.pop()\n",
        "    next_state = copy(self.state)\n",
        "    self.observation, self.reward, self.is_done,_ = next_state.step(action)\n",
        "    child_node = Node(next_state, parent = self)\n",
        "    child_node.action = action\n",
        "    self.children.append(child_node)\n",
        "    return child_node\n",
        "  \n",
        "  def rollout_policy(self, state):\n",
        "    return state.action_space.sample()\n",
        "  \n",
        "  def rollout(self, t_max = 10**8):\n",
        "    state = copy(self.state)\n",
        "    rollout_return = 0\n",
        "    gamma = 0.6\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = self.rollout_policy(state)\n",
        "      obs, reward, done, _ = state.step(action)\n",
        "      rollout_return += gamma * reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    return rollout_return\n",
        "\n",
        "  def backpropagate(self, child_value):\n",
        "    node_value = self.reward + child_value\n",
        "    self.q += node_value\n",
        "    self.visiting_times += 1\n",
        "    if self.parent:\n",
        "      return self.parent.backpropagate(node_value)\n",
        "\n",
        "\n",
        "class MonteCarloTreeSearch(object):\n",
        "  def __init__(self, node):\n",
        "    self.root = node\n",
        "\n",
        "  def best_action(self, simulations_number):\n",
        "    for _ in range(0, simulations_number):\n",
        "      v = self._tree_policy()\n",
        "      reward = v.rollout()\n",
        "      v.backpropagate(reward)\n",
        "    return self.root.best_child()\n",
        "\n",
        "  def _tree_policy(self):\n",
        "    current_node = self.root\n",
        "    while not current_node.is_terminal_node():\n",
        "      if not current_node.is_fully_expanded():\n",
        "        return current_node.expand()\n",
        "      else:\n",
        "        current_node = current_node.best_child()\n",
        "    return current_node"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEaUyWElVd92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "700c1f65-22f2-41cb-df83-7407afa4a08d"
      },
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "n_simulation = 10**4\n",
        "root = Node(env)\n",
        "is_done = False\n",
        "total_reward, penalty, epochs = 0, 0, 0\n",
        "\n",
        "while not is_done:\n",
        "  mcts = MonteCarloTreeSearch(root)\n",
        "  best_child = mcts.best_action(n_simulation)\n",
        "  new_state, reward, is_done, info = env.step(best_child.action)\n",
        "  total_reward += reward\n",
        "  if reward == -10:\n",
        "    penalty += 1\n",
        "  epochs += 1\n",
        "  root = best_child\n",
        "\n",
        "env.render()\n",
        "print('Timesteps taken:', epochs)\n",
        "print('Penalty:', penalty)\n",
        "print('total_reward:', total_reward)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Timesteps taken: 34\n",
            "Penalty: 13\n",
            "total_reward: -130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCQtUjRAodob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}